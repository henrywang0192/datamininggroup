{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter name(s) here: Andrew Vuong, Henry Wang, Aaron Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project\n",
    "\n",
    "Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python. In this assigment you'll explore how to train various classifiers using the `scikit-learn` library. The scikit-learn documentation can be found [here](http://scikit-learn.org/stable/documentation.html).\n",
    "\n",
    "In this assignment we'll attempt to classify patients as either having or not having diabetic retinopathy, using the same Diabetic Retinopathy data set from your previous assignments. Recall that this dataset contains 1151 records and 20 attributes (some categorical, some continuous). You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You may add additional imports\n",
    "import warnings\n",
    "#warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Age    SEE_01    SEE_16  \\\n",
      "study_gr study_gr1 Diag_cyt Diag_cyt_rev                                  \n",
      "1        1         -1       -1            29.500000       NaN       NaN   \n",
      "                    0        0            31.565652  0.000000  0.000000   \n",
      "2        2          0        0            31.639831  0.000000  0.000000   \n",
      "         3          0        1            29.166667  1.000000  0.791667   \n",
      "                             2            29.000000  1.000000  1.000000   \n",
      "                             4            28.500000  1.000000  0.500000   \n",
      "                             5            39.000000  1.000000  0.000000   \n",
      "3        4          1        1            29.870270  0.248322  0.127517   \n",
      "                    2        2            29.109756  0.484848  0.212121   \n",
      "4        5          3        3            31.085714  0.909091  0.818182   \n",
      "                    4        4            29.375000  0.933333  0.733333   \n",
      "                    5        5            35.000000       NaN       NaN   \n",
      "                    6        6            30.000000       NaN       NaN   \n",
      "\n",
      "                                            SEE_18    SEE_45  Ind_biop  \\\n",
      "study_gr study_gr1 Diag_cyt Diag_cyt_rev                                 \n",
      "1        1         -1       -1                 NaN       NaN  0.500000   \n",
      "                    0        0            0.000000  0.000000  0.035431   \n",
      "2        2          0        0            0.000000  0.000000  0.024011   \n",
      "         3          0        1            0.250000  0.041667  0.666667   \n",
      "                             2            0.000000  0.000000  1.000000   \n",
      "                             4            0.500000  0.000000  1.000000   \n",
      "                             5            1.000000  0.000000  1.000000   \n",
      "3        4          1        1            0.120805  0.013423  0.335135   \n",
      "                    2        2            0.181818  0.090909  0.500000   \n",
      "4        5          3        3            0.090909  0.000000  1.000000   \n",
      "                    4        4            0.266667  0.133333  1.000000   \n",
      "                    5        5                 NaN       NaN  1.000000   \n",
      "                    6        6                 NaN       NaN  1.000000   \n",
      "\n",
      "                                          biop_status  \n",
      "study_gr study_gr1 Diag_cyt Diag_cyt_rev               \n",
      "1        1         -1       -1               4.000000  \n",
      "                    0        0               0.240517  \n",
      "2        2          0        0               0.137712  \n",
      "         3          0        1              14.250000  \n",
      "                             2              23.000000  \n",
      "                             4              22.000000  \n",
      "                             5              26.000000  \n",
      "3        4          1        1               6.524324  \n",
      "                    2        2              10.317073  \n",
      "4        5          3        3              21.114286  \n",
      "                    4        4              22.406250  \n",
      "                    5        5              20.000000  \n",
      "                    6        6              26.000000  \n",
      "    Age  study_gr  study_gr1  Diag_cyt  Diag_cyt_rev  SEE_01  SEE_16  SEE_18  \\\n",
      "0    34         1          1         0             0     0.0     0.0     0.0   \n",
      "1    31         1          1         0             0     0.0     0.0     0.0   \n",
      "2    25         1          1         0             0     0.0     0.0     0.0   \n",
      "3    33         1          1         0             0     0.0     0.0     0.0   \n",
      "4    27         1          1         0             0     0.0     0.0     0.0   \n",
      "..  ...       ...        ...       ...           ...     ...     ...     ...   \n",
      "95   33         1          1         0             0     0.0     0.0     0.0   \n",
      "96   30         1          1         0             0     0.0     0.0     0.0   \n",
      "97   29         1          1         0             0     0.0     0.0     0.0   \n",
      "98   26         1          1         0             0     0.0     0.0     0.0   \n",
      "99   30         1          1         0             0     0.0     0.0     0.0   \n",
      "\n",
      "    SEE_45  Ind_biop  biop_status  \n",
      "0      0.0         0           26  \n",
      "1      0.0         1           23  \n",
      "2      0.0         1           23  \n",
      "3      0.0         0           23  \n",
      "4      0.0         1           23  \n",
      "..     ...       ...          ...  \n",
      "95     0.0         1            4  \n",
      "96     0.0         1            4  \n",
      "97     0.0         1            4  \n",
      "98     0.0         0            4  \n",
      "99     0.0         1            4  \n",
      "\n",
      "[100 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the data from csv file\n",
    "\n",
    "data = pd.read_csv(\"Quality_control_cervical_cytology.csv\", delimiter='\\t', header=0)\n",
    "# print(data.head(10))\n",
    "\n",
    "#     data.set_value(i, 'age',100)\n",
    "#     data.loc[i,'age'] = 100\n",
    "#     data.ix[i,'age'] = 100\n",
    "#     data.iloc[i, 'age'] = 100\n",
    "#     print(row[0])\n",
    "# for i in data['age']:\n",
    "#     print(i)\n",
    "# print(data.head(10))\n",
    "# def change_neg_to_nan(x):\n",
    "#     if x == -1:\n",
    "#         x = n\n",
    "# data[['SEE_01']] = data.apply(change_neg_to_nan, axis=1)\n",
    "\n",
    "df_full = data.groupby(['study_gr', 'study_gr1', 'Diag_cyt', 'Diag_cyt_rev'])\n",
    "test = list(df_full.groups[4,5,3,3])\n",
    "\n",
    "df_full_prob = df_full.mean()\n",
    "print(df_full_prob)\n",
    "# print(df_full_prob['SEE_01'][2][3])\n",
    "# print(df_full_prob.keys())\n",
    "# print(df_full_prob['SEE_01'])\n",
    "\n",
    "# for k in df_full_prob.keys():\n",
    "#     print(k)\n",
    "\n",
    "# test1 = list(df_full.groups[1,1,-1,-1])\n",
    "# print(test)\n",
    "count = 0\n",
    "for i,row in data.iterrows():\n",
    "    if row[5] != row[5]:\n",
    "#         print(df_full_prob['SEE_01'][row[1]][row[2]][row[3]][row[4]])\n",
    "        see_01_roll = random.uniform(0, 1)\n",
    "        see_16_roll = random.uniform(0, 1)\n",
    "        see_18_roll = random.uniform(0, 1)\n",
    "        see_45_roll = random.uniform(0, 1)\n",
    "        see_01_val = 1 if see_01_roll < df_full_prob['SEE_01'][row[1]][row[2]][row[3]][row[4]] else 0\n",
    "        see_16_val = 1 if see_16_roll < df_full_prob['SEE_16'][row[1]][row[2]][row[3]][row[4]] else 0\n",
    "        see_18_val = 1 if see_18_roll < df_full_prob['SEE_18'][row[1]][row[2]][row[3]][row[4]] else 0\n",
    "        see_45_val = 1 if see_45_roll < df_full_prob['SEE_45'][row[1]][row[2]][row[3]][row[4]] else 0\n",
    "        \n",
    "        data.at[i,'SEE_01'] = see_01_val\n",
    "        data.at[i,'SEE_16'] = see_16_val\n",
    "        data.at[i,'SEE_18'] = see_18_val\n",
    "        data.at[i,'SEE_45'] = see_45_val\n",
    "        count += 1\n",
    "print(data.head(100))\n",
    "data.to_csv(r'filledmissing.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4366, 10)\n",
      "   Age  study_gr  study_gr1  Diag_cyt  Diag_cyt_rev  SEE_01  SEE_16  SEE_18  \\\n",
      "0   34         1          1         0             0     0.0     0.0     0.0   \n",
      "1   31         1          1         0             0     0.0     0.0     0.0   \n",
      "2   25         1          1         0             0     0.0     0.0     0.0   \n",
      "3   33         1          1         0             0     0.0     0.0     0.0   \n",
      "4   27         1          1         0             0     0.0     0.0     0.0   \n",
      "5   24         1          1         0             0     0.0     0.0     0.0   \n",
      "6   30         1          1         0             0     0.0     0.0     0.0   \n",
      "7   26         1          1         0             0     0.0     0.0     0.0   \n",
      "8   30         1          1         0             0     0.0     0.0     0.0   \n",
      "9   38         1          1         0             0     0.0     0.0     0.0   \n",
      "\n",
      "   SEE_45  Ind_biop  \n",
      "0     0.0         0  \n",
      "1     0.0         1  \n",
      "2     0.0         1  \n",
      "3     0.0         0  \n",
      "4     0.0         1  \n",
      "5     0.0         1  \n",
      "6     0.0         1  \n",
      "7     0.0         1  \n",
      "8     0.0         1  \n",
      "9     0.0         1  \n",
      "[1 1 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "dataonly = data.drop(['biop_status'], axis=1)\n",
    "# print(dataonly)\n",
    "labels = data['biop_status'].apply(lambda x: 1 if int(x) > 20 else 0).ravel()\n",
    "# labels = data['biop_status'].values.ravel()\n",
    "# print(labels)\n",
    "\n",
    "print(dataonly.shape)\n",
    "print(dataonly.head(10))\n",
    "print(labels)\n",
    "##################################################################################\n",
    "# labels = data['biop_status'].apply(x: 1 if int(x) > 20 else 0).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Decision Trees (DT) & Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. You can train a classifier using the holdout method by splitting your data into a  training set and a  test set, then you can evaluate the classifier on the held-out test set. \n",
    "\n",
    "Let's try this with a decision tree classifier. \n",
    "\n",
    "* Use `sklearn.model_selection.train_test_split` to split your dataset into training and test sets (do an 80%-20% split). Display how many records are in the training set and how many are in the test set.\n",
    "* Use `sklearn.tree.DecisionTreeClassifier` to fit a decision tree classifier on the training set. Use entropy as the split criterion. \n",
    "* Now that the tree has been learned from the training data, we can run the test data through and predict classes for the test data. Use the `predict` method of `DecisionTreeClassifier` to classify the test data. \n",
    "* Then use `sklearn.metrics.accuracy_score` to print out the accuracy of the classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3492, 10)\n",
      "(874, 10)\n",
      "(3492,)\n",
      "(874,)\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.model_selection import train_test_split  #, DecisionTreeClassifier\n",
    "train_x, test_x, train_y, test_y = train_test_split(dataonly, labels, test_size=0.2, random_state=42)\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Note that the DecisionTree classifier has many parameters that can be set. Try tweaking parameters like split criterion, max_depth, min_impurity_decrease, min_samples_leaf, min_samples_split, etc. to see how they affect accuracy. Print the accuracy of a few different variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for random state=0 and using gini: 0.9508009153318078\n",
      "for random state=0 and using entropy: 0.9519450800915332\n",
      "for random state=42 and using gini: 0.9508009153318078\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dtree = DecisionTreeClassifier(random_state=0, criterion='gini') \n",
    "dtree.fit(train_x, train_y)\n",
    "prediction = dtree.predict(test_x)\n",
    "print(\"for random state=0 and using gini: {}\".format(accuracy_score(prediction, test_y)))\n",
    "dtree = DecisionTreeClassifier(random_state=0, criterion='entropy') \n",
    "dtree.fit(train_x, train_y)\n",
    "prediction = dtree.predict(test_x)\n",
    "print(\"for random state=0 and using entropy: {}\".format(accuracy_score(prediction, test_y)))\n",
    "dtree = DecisionTreeClassifier(random_state=42, criterion='gini') \n",
    "dtree.fit(train_x, train_y)\n",
    "prediction = dtree.predict(test_x)\n",
    "print(\"for random state=42 and using gini: {}\".format(accuracy_score(prediction, test_y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. You have now built a decision tree and tested it's accuracy using the \"holdout\" method. But as discussed in class, this is not sufficient for estimating generalization accuracy. Instead, we should use Cross Validation to get a better estimate of accuracy. \n",
    "\n",
    "Use `sklearn.model_selection.cross_val_score` to perform 10-fold cross validation on a decision tree. You will pass the FULL dataset into `cross_val_score` which will automatically divide it into the number of folds you tell it to, train a decision tree model on the training set for each fold, and test it on the test set for each fold. It will return a numpy array with the accuracy out of each fold. Average these accuracies to print out the generalization accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9109589  0.95205479 0.95423341 0.96567506 0.95642202 0.93119266\n",
      " 0.9587156  0.98165138 0.98853211 0.18348624]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(dtree, dataonly, labels, cv=10, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nested Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Now we want to tune our model to use the best parameters to avoid overfitting to our training data. Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters (hyperparameters) specified in a grid. \n",
    "* Use `sklearn.model_selection.GridSearchCV` to find the best `max_depth`, `max_features`, and `min_samples_leaf` for your tree. Use a 5-fold-CV and 'accuracy' for the scoring criteria.\n",
    "* Try the values [5,10,15,20] for `max_depth` and `min_samples_leaf`. Try [5,10,15] for `max_features`. \n",
    "* Print out the best value for each of the tested parameters (`best_params_`).\n",
    "* Print out the accuracy of the model with these best values (`best_score_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.21.3\n",
      "\n",
      "best parameters:\n",
      "max_depth: 5\n",
      "max_features: 6\n",
      "min_samples_leaf: 20\n",
      "\n",
      "best score: 0.9627720504009164\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm, datasets\n",
    "max_depth = [5,10,15,20]\n",
    "min_samples_leaf = [5,10,15,20]\n",
    "max_features = [2,4,6,8,10]\n",
    "param_grid = {'max_depth':max_depth, 'min_samples_leaf': min_samples_leaf, 'max_features':max_features}\n",
    "cv = 5\n",
    "dtreeGSCV = GridSearchCV(dtree,param_grid,cv=cv)\n",
    "dtreeGSCV.fit(train_x, train_y)\n",
    "print('The scikit-learn version is {}\\n'.format(sk.__version__))\n",
    "print(\"best parameters:\\nmax_depth: {}\\nmax_features: {}\\nmin_samples_leaf: {}\\n\".format(\\\n",
    "dtreeGSCV.best_params_['max_depth'],\\\n",
    "dtreeGSCV.best_params_['max_features'],\\\n",
    "dtreeGSCV.best_params_['min_samples_leaf']))\n",
    "print(\"best score: {}\".format(dtreeGSCV.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What you did in Q5 performed the _inner_ loop of a nested CV (no test set was held out). What you did in Q4 performed an _outer_ loop of CV (holds out a test set). Now we need to combine them to perform the nested cross-validation that we discussed in class. To do this, you'll need to pass the a `GridSearchCV` into a `cross_val_score`. \n",
    "\n",
    "What this does is: the `cross_val_score` splits the data in to train and test sets for the first outer fold, and it passes the train set into `GridSearchCV`. `GridSearchCV` then splits that set into train and validation sets for k number of folds (the inner CV loop). The hyper-parameters for which the average score over all inner iterations is best, is reported as the `best_params_`, `best_score_`, and `best_estimator_`(best decision tree). This best decision tree is then evaluated with the test set from the `cross_val_score` (the outer CV loop). And this whole thing is repeated for the remaining k folds of the `cross_val_score` (the outer CV loop). \n",
    "\n",
    "That is a lot of explanation for a very complex (but IMPORTANT) process, which can all be performed with a single line of code!\n",
    "\n",
    "Be patient for this one to run. The nested cross-validation loop can take some time. A [ * ] next to the cell indicates that it is still running.\n",
    "\n",
    "Print the accuracy of your tuned, cross-validated model. This is the official accuracy that you would report for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96571429 0.94857143 0.96275072 0.95988539 0.96275072 0.95702006\n",
      " 0.96275072 0.96275072 0.94842407 0.9713467 ]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "print(cross_val_score(dtreeGSCV, train_x, train_y, cv=10, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Naive Bayes (NB) & Evaluation Metrics\n",
    "\n",
    "`sklearn.naive_bayes.GaussianNB` implements the Gaussian Naive Bayes algorithm for classification. This means that the liklihood of continuous features is estimated using a Gaussian distribution. (Refer to slide 13 of the Naive Bayes powerpoint notes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Create a `sklearn.naive_bayes.GaussianNB` classifier. Use `sklearn.model_selection.cross_val_score` to do a 10-fold cross validation on the classifier. Display the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95714286 0.93428571 0.94555874 0.93409742 0.94555874 0.93123209\n",
      " 0.93696275 0.94842407 0.93696275 0.9512894 ]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.naive_bayes import GaussianNB # then cross_val_score\n",
    "modelNB = GaussianNB()\n",
    "modelNB.fit(train_x, train_y)\n",
    "print(cross_val_score(modelNB, train_x, train_y, cv=10, n_jobs=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. `cross_val_score` returns the scores of every test fold. There is another function called `cross_val_predict` that returns predicted y values for every record in the test fold. In other words, for each element in the input, `cross_val_predict` returns the prediction that was obtained for that element when it was in the test set. \n",
    "\n",
    "* Use `cross_val_predict` and `sklearn.metrics.confusion_matrix` to print the confusion matrix for the classifier.\n",
    "\n",
    "* Sckit-learn also provides a useful function `sklearn.metrics.classification_report` for evaluating the classifier on a per-class basis. It is a summary of the precision, recall, and F1 score for each class (and support is just the actual class count). Display the classification report for your Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[3145  157]\n",
      " [  45  145]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      3302\n",
      "           1       0.48      0.76      0.59       190\n",
      "\n",
      "    accuracy                           0.94      3492\n",
      "   macro avg       0.73      0.86      0.78      3492\n",
      "weighted avg       0.96      0.94      0.95      3492\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "predict = cross_val_predict(modelNB, train_x, train_y, cv=10, n_jobs=-1)\n",
    "true = train_y\n",
    "print(\"Confusion Matrix:\\n{}\\n\".format(confusion_matrix(true, predict)))\n",
    "print(\"Classification Report:\\n{}\\n\".format(classification_report(true, predict)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Using `sklearn.metrics.roc_curve` plot a ROC curve for the Naive Bayes classifier. Also calculate the area under the curve (AUC) using `sklearn.metrics.roc_auc_score`.\n",
    "\n",
    "* We will just do this on a single holdout test set (because it gets more complicated to put this inside of a cross-validation). So, split your data into training and test sets using `sklearn.model_selection.train_test_split`. Do an 80/20 split.\n",
    "* Fit the Naive Bayes classifier to the training data by calling the `fit` method on the trainng data.\n",
    "* Now call the `predict_proba` method on your classifier and pass in the test data. This will return a 2D numpy array with one row for each datapoint in the test set and 2 columns. Column index 0 is the probability that this datapoint is in class 0, and column index 1 is the probability that this datapoint is in class 1.\n",
    "* We are going to say that class 1 (having the disease) is the rare/positive class. To create a ROC curve, pass the actual Y labels and the probabilites of class 1 (column index 1 out of your predict_proba result) into `sklearn.metrics.roc_curve`\n",
    "* Pass the FPR and TPR that `roc_curve` returns into the plotting code that we have provided you.\n",
    "* Print the AUC (area under the curve) by using `sklearn.metrics.roc_auc_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.9467962760131435\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm.libsvm import predict_proba\n",
    "# your code goes here\n",
    "# train_x, test_x, train_y, test_y split already above\n",
    "\n",
    "modelNB.fit(train_x, train_y)\n",
    "\n",
    "predict_proba_results = modelNB.predict_proba(test_x)\n",
    "# print(predict_proba_results)\n",
    "\n",
    "#replace these fpr and tpr with the results of your roc_curve\n",
    "fpr, tpr, thres = [], [], []\n",
    "fpr, tpr, thres = roc_curve(test_y, predict_proba_results[:,1])\n",
    "# print(predict_proba_results[:,1])\n",
    "# print((predict_proba_results))\n",
    "print(\"roc_auc_score: {}\".format(roc_auc_score(test_y, predict_proba_results[:,1])))\n",
    "# Do not change this code! This plots the ROC curve.\n",
    "# Just replace the fpr and tpr above with the values from your roc_curve\n",
    "plt.plot([0,1],[0,1],'k--') #plot the diagonal line\n",
    "plt.plot(fpr, tpr, label='NB') #plot the ROC curve\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title('ROC Curve Naive Bayes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. k-Nearest Neighbor (KNN) & Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some classification algorithms, scaling of the data is critical (like KNN, SVM, Neural Nets). For other classification algorithms, data scaling is not necessary (like Naive Bayes and Decision Trees). _Take a minute to think about why this is the case!!_ But using scaled data with an algorithm that doesn't explicitly need it to be scaled does not hurt the results of that algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. The distance calculation method is central to the KNN algorithm. By default, `KNeighborsClassifier` uses  Euclidean distance as its metric (but this can be changed). Because of the distance calculations, it is critical to scale the data before running Nearest Neighbor!\n",
    "\n",
    "We discussed why dimensionality reduction may also be needed with KNN because of the curse of dimensionality. So we may want to also perform a dimensionality reduction with PCA before running KNN. PCA should only be performed on scaled data! (Remember that you can also reduce dimensionality by performing feature selection and feature engineering.) \n",
    "\n",
    "An important note about scaling data and dimensionality reduction is that they should only be performed on the **training** data, then you transform the test data into the scaled, PCA space that was found on the training data. (Refer to the concept of [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/).)\n",
    "\n",
    "So when you are doing cross-validation, the scaling and PCA needs to happen *inside of your CV loop*. This way, it is performed on the training set for the first fold, then the test set is put into that space. On the second fold, it is performed on the trainng set for the second fold, and the test set is put into that space. And so on for the remaining folds. \n",
    "\n",
    "In order to do this with scikit-learn, you must create what's called a `Pipeline` and pass that in to the cross validation. This is a very important concept for Data Mining and Machine Learning, so let's practice it here.\n",
    "\n",
    "Do the following:\n",
    "* Create a `sklearn.preprocessing.StandardScaler` object to standardize the datasetâ€™s features (mean = 0 and variance = 1). (Do not call `fit` on it yet. Just create the `StandardScaler` object.)\n",
    "* Create a `sklearn.decomposition.PCA` object to perform PCA dimensionality reduction. (Do not call `fit` on it yet. Just create the `PCA` object.)\n",
    "* Create a `sklearn.neighbors.KNeighborsClassifier`. The number of neighbors defaults to 5 (k=5). Go ahead and change it to 7. (Do not call `fit` on it yet. Just create the `KNeighborsClassifier` object.)\n",
    "* Create a `sklearn.pipeline.Pipeline` object and set the `steps` to the scaler, the PCA, and the KNN objects that you just created. \n",
    "* Pass the `pipeline` object in to a `cross_val_score` as the estimator, along with the features and the labels, and use a 5-fold-CV. \n",
    "\n",
    "In each fold of the cross validation, the training phase will use _only_ the training data for scaling, PCA, and training the model. Then the testing phase will scale & transform the test data into the PCA space (found on the training data) and run the test data through the trained classifier, to return an accuracy measurement for each fold. Print the average accuracy across all 5 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96280401 0.95851216 0.96275072 0.95558739 0.95988539]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "knc = KNeighborsClassifier(n_neighbors=7)\n",
    "pipe = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "                       ('pca', PCA()), \n",
    "                       ('knc', KNeighborsClassifier(n_neighbors=7))])\n",
    "print(cross_val_score(pipe, train_x, train_y, cv=5, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. Another important part of KNN is choosing the best number of neighbors (tuning the hyperparameter, k). We can use nested cross validation to do this. Let's try k values from 1-25 to find the best one. \n",
    "\n",
    "We _also_ want to find the best number of dimensions to project down onto using PCA. We can use nested cross validation to do this as well. Let's try from 5-19 dimensions.\n",
    "\n",
    "* Starter code is provided to create the \"parameter grid\" to search. You will need to change this code! Where I have \"knn__n_neighbors\", this indicates that I want to tune the \"n_neighbors\" parameter in the \"knn\" part of the pipeline. When you created your pipeline above, you named the KNN part of the pipeline with a string. You should replace \"knn\" in the param_grid below with whatever you named your KNN part of the pipeline: **<replace_this>__n_neighbors.** Do the same for the PCA part of the pipeline.\n",
    "* Create a `sklearn.model_selection.GridSearchCV` and pass in the pipeline, the param_grid, and set it to a 5-fold-CV.\n",
    "* Now, on that `GridSearchCV` object, call `fit` and pass in the features and labels.\n",
    "* Show the best number of dimensions and best number of neighbors for this dataset by printing the `best_params_` from the `GridSearchCV`.\n",
    "* Also print the accuracy when using this best number of dimensions and neighbors by printing the `best_score_` from the `GridSearchCV`.\n",
    "\n",
    "Be patient, this can take some time to run. It is trying every combination of dimensions from 5-19 with every k from 1-25! A [ * ] next to the cell indicates that it is still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.21.3\n",
      "\n",
      "best parameters:\n",
      "'pca_ncomponents': 14\n",
      "'knc_nneighbors': 20\n",
      "\n",
      "best score: 0.6478260869565218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/forever/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "On the \"pca\" part of the pipeline, \n",
    "tune the n_components parameter,\n",
    "by trying the values 1-19.\n",
    "\n",
    "On the \"knn\" part of the pipeline, \n",
    "tune the n_neighbors parameter,\n",
    "by trying the values 1-30.\n",
    "'''\n",
    "# print(pipe.get_params().keys())\n",
    "param_grid = {\n",
    "    'pca__n_components': list(range(5, 19)),\n",
    "    'knc__n_neighbors': list(range(1, 25))\n",
    "}\n",
    "\n",
    "# your code goes here\n",
    "cv = 5\n",
    "knnGSCV = GridSearchCV(pipe,param_grid,cv=cv)\n",
    "knnGSCV.fit(train_x, train_y)\n",
    "print('The scikit-learn version is {}\\n'.format(sk.__version__))\n",
    "print(\"best parameters:\\n'pca_ncomponents': {}\\n'knc_nneighbors': {}\\n\".format(\\\n",
    "knnGSCV.best_params_['pca__n_components'],\\\n",
    "knnGSCV.best_params_['knc__n_neighbors']))\n",
    "print(\"best score: {}\".format(knnGSCV.best_score_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12. In Q11, we did not hold out a test set. The accuracy reported out is on the _validation_ set. So now we need to wrap the whole process in another cross-validation to perform a nested cross-validation and report the accuarcy of this KNN model on unseen test data. This is the official accuracy you would report on this model.\n",
    "\n",
    "You'll need to pass the `GridSearchCV` into a `cross_val_score`, just as you did with the decision tree. Use a 5-fold-CV for the outer loop. \n",
    "\n",
    "Again, be patient for this one to run. The nested cross-validation loop can take some time. It is doing what it did above in Q11 five times. A [ * ] next to the cell indicates that it is still running. (Just for comparison, mine takes about 2 mins to run and the fan revs up so it sounds like my computer is going to explode. All computers are different, so yours could take shorter or longer...)\n",
    "\n",
    "<img src=\"model_is_training.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67567568 0.58378378 0.58152174 0.66120219 0.68852459]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "print(cross_val_score(knnGSCV, train_x, train_y, cv=5, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13. Now put it all together with an SVM. \n",
    "* Create a `pipeline` that includes scaling, PCA, and an `sklearn.svm.SVC`.\n",
    "* Create a parameter grid that tries number of dimensions from 5-19 and SVM kernels `linear`, `rbf` and `poly`.\n",
    "* Create a `GridSearchCV` for the inner CV loop. Use a 5-fold CV.\n",
    "* Run a `cross_val_predict` with a 10-fold CV for the outer loop. \n",
    "* Print out the accuracy and the classification report of using an SVM classifier on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.86      0.74       437\n",
      "           1       0.82      0.60      0.69       483\n",
      "\n",
      "    accuracy                           0.72       920\n",
      "   macro avg       0.74      0.73      0.72       920\n",
      "weighted avg       0.74      0.72      0.72       920\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.svm import SVC\n",
    "warnings.simplefilter(\"ignore\")\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "svc = SVC()\n",
    "pipe = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "                       ('pca', PCA()), \n",
    "                       ('svc', SVC())])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': list(range(5, 19)),\n",
    "    'svc__kernel': ['linear','rbf','poly']\n",
    "}\n",
    "\n",
    "# # your code goes here\n",
    "cv = 5\n",
    "svcGSCV = GridSearchCV(pipe,param_grid,cv=cv)\n",
    "svcGSCV.fit(train_x, train_y)\n",
    "\n",
    "predict = cross_val_predict(svcGSCV, train_x, train_y, cv=10, n_jobs=-1)\n",
    "true = train_y\n",
    "\n",
    "print(\"Classification Report:\\n{}\\n\".format(classification_report(true, predict)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Neural Networks (NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14. Train a multi-layer perceptron with a single hidden layer using `sklearn.neural_network.MLPClassifier`. \n",
    "* Create a pipeline with scaling and a neural net. (No PCA on this one. But scaling is critical to neural nets.)\n",
    "* Use `GridSearchCV` with 5 fold cross validation to find the best hidden layer size and the best activation function. \n",
    "* Try values of `hidden_layer_sizes` ranging from `(30,)` to `(60,)` by increments of 10.\n",
    "* Try activation functions `logistic`, `tanh`, `relu`.\n",
    "* Wrap your `GridSearchCV` in a 5-fold `cross_val_score` and report the accuracy of your neural net.\n",
    "\n",
    "Be patient, as this can take a few minutes to run. You may get ConvergenceWarnings as it runs - that is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.21.3\n",
      "\n",
      "best score: 0.7358695652173913\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "scaler = StandardScaler()\n",
    "mlp = MLPClassifier()\n",
    "pipe = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "                       ('mlp', MLPClassifier())])\n",
    "\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': list(range(30,60,10)),\n",
    "    'mlp__activation': ['logistic', 'tanh', 'relu']\n",
    "}\n",
    "\n",
    "# # # your code goes here\n",
    "cv = 5\n",
    "mlpGSCV = GridSearchCV(pipe,param_grid,cv=cv)\n",
    "mlpGSCV.fit(train_x, train_y)\n",
    "print('The scikit-learn version is {}\\n'.format(sk.__version__))\n",
    "\n",
    "print(\"best score: {}\".format(mlpGSCV.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Ensemble Classifiers\n",
    "\n",
    "Ensemble classifiers combine the predictions of multiple base estimators to improve the accuracy of the predictions. One of the key assumptions that ensemble classifiers make is that the base estimators are built independently (so they are diverse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forests**\n",
    "\n",
    "Q15. Use `sklearn.ensemble.RandomForestClassifier` to classify the data. Scaling the data is not necessary for Decision Trees (take a minute to think about why). So, no need for a pipeline here.\n",
    "\n",
    "Use a `GridSearchCV` with a 5-fold CV to tune the hyperparameters to get the best results. \n",
    "* Try `max_depth` ranging from 35-55\n",
    "* Try `min_samples_leaf` of 8, 10, 12\n",
    "* Try `max_features` of `\"sqrt\"` and `\"log2\"`\n",
    "\n",
    "Wrap your GridSearchCV in a cross_val_score with 5-fold CV to report the accuracy of the model.\n",
    "\n",
    "Be patient, this can take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.21.3\n",
      "\n",
      "best parameters:\n",
      "max_depth: 35\n",
      "max_features: sqrt\n",
      "min_samples_leaf: 12\n",
      "\n",
      "cross val score: [0.63783784 0.67567568 0.61956522 0.59016393 0.6284153 ]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "max_depth = list(range(35,56))\n",
    "min_samples_leaf = [8, 10, 12]\n",
    "max_features = ['sqrt', 'log2']\n",
    "param_grid = {'max_depth':max_depth, 'min_samples_leaf': min_samples_leaf, 'max_features':max_features}\n",
    "cv = 5\n",
    "rfcGSCV = GridSearchCV(dtree,param_grid,cv=cv)\n",
    "rfcGSCV.fit(train_x, train_y)\n",
    "print('The scikit-learn version is {}\\n'.format(sk.__version__))\n",
    "print(\"best parameters:\\nmax_depth: {}\\nmax_features: {}\\nmin_samples_leaf: {}\\n\".format(\\\n",
    "rfcGSCV.best_params_['max_depth'],\\\n",
    "rfcGSCV.best_params_['max_features'],\\\n",
    "rfcGSCV.best_params_['min_samples_leaf']))\n",
    "print(\"cross val score: {}\".format(cross_val_score(rfcGSCV, train_x, train_y, cv=5, n_jobs=-1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost**\n",
    "\n",
    "Random Forests are a kind of averaging ensemble classifier, where several estimators are built independently and then to average their predictions (by taking a vote). There is another method of training ensemble classifiers called *boosting*. Here the classifiers are trained sequentially and each time the sampling of the training set depends on the performance of previously generated models.\n",
    "\n",
    "Q16. Evaluate a `sklearn.ensemble.AdaBoostClassifier` classifier on the data. By default, `AdaBoostClassifier` uses decision trees as the base classifiers (but this can be changed). Use 150 base classifiers to make an `AdaBoostClassifier` and evaluate it's accuracy with a 5-fold-CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63783784 0.67567568 0.66304348 0.6557377  0.68306011]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=150, random_state=0)\n",
    "ada.fit(train_x, train_y)  \n",
    "print(cross_val_score(ada, train_x, train_y, cv=5, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Build your final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have tested all kinds of classifiers on this data. Some have performed better than others. \n",
    "\n",
    "Q17. We may not want to deploy any of these models in the real world to actually diagnose patients because the accuracies are not high enough. What can we do to improve the accuracy rates? Answer as a comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo improve accuracy rates:\\nWe should have a larger dataset to train on.\\nMaintain Cross Validations.\\nData augmentation to create larger dataset.\\nFeature selection / engineering.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Answer here as a comment.\n",
    "'''\n",
    "\"\"\"\n",
    "To improve accuracy rates:\n",
    "We should have a larger dataset to train on.\n",
    "Maintain Cross Validations.\n",
    "Data augmentation to create larger dataset.\n",
    "Feature selection / engineering.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q18. Let's say we *did* get to the point where we had a model with very high accuracy and we want to deploy that model and use it for real-world predictions.\n",
    "\n",
    "* Let's say we're going to deploy our SVM classifier.\n",
    "* We need to make one final version of this model, where we use ALL of our available data for training (we do not hold out a test set this time, so no outer cross-validation loop). \n",
    "* We need to tune the parameters of the model on the FULL dataset, so copy the code you entered for Q13, but remove the outer cross validation loop (remove `cross_val_score`). Just run the `GridSearchCV` by calling `fit` on it and passing in the full dataset. This results in the final trained model with the best parameters for the full dataset. You can print out `best_params_` to see what they are.\n",
    "* The accuracy of this model is what you assessed and reported in Q13.\n",
    "\n",
    "\n",
    "* Use the `pickle` package to save your model. We have provided the lines of code for you, just make sure your final model gets passed in to `pickle.dump()`. This will save your model to a file called finalized_model.sav in your current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:\n",
      "'pca__n_components': 18\n",
      "'svc__kernel': linear\n",
      "\n",
      "best score: 0.7254561251086012\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# your code goes here\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "warnings.simplefilter(\"ignore\")\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "svc = SVC()\n",
    "pipe = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "                       ('pca', PCA()), \n",
    "                       ('svc', SVC())])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': list(range(5, 19)),\n",
    "    'svc__kernel': ['linear','rbf','poly']\n",
    "}\n",
    "\n",
    "# # your code goes here\n",
    "cv = 5\n",
    "svcGSCV = GridSearchCV(pipe,param_grid,cv=cv)\n",
    "svcGSCV.fit(dataonly, labels)\n",
    "\n",
    "print(\"best parameters:\\n'pca__n_components': {}\\n'svc__kernel': {}\\n\".format(\\\n",
    "svcGSCV.best_params_['pca__n_components'],\\\n",
    "svcGSCV.best_params_['svc__kernel']))\n",
    "print(\"best score: {}\".format(svcGSCV.best_score_))\n",
    "\n",
    "#replace this final_model with your final model\n",
    "final_model = svcGSCV\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(final_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q19. Now if someone wants to use your trained, saved classifier to classify a new record, they can load the saved model and just call predict on it. \n",
    "* Given this new record, classify it with your saved model and print out either \"Negative for disease\" or \"Positive for disease.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive for disease\n"
     ]
    }
   ],
   "source": [
    "# some time later...\n",
    "from numpy import array\n",
    "# use this as the new record to classify\n",
    "record = [ 0.05905386, 0.2982129, 0.68613149, 0.75078865, 0.87119216, 0.88615694,\n",
    "  0.93600623, 0.98369184, -0.47426472, -0.57642756, -0.53115361, -0.42789774,\n",
    " -0.21907738, -0.20090532, -0.21496782, -0.2080998, 0.06692373, -2.81681183,\n",
    " -0.7117194 ]\n",
    "\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open('/u/forever/fall2019/dm/assignment/3/finalized_model.sav', 'rb'))\n",
    "result = loaded_model.predict(array(record).reshape(1, -1))\n",
    "\n",
    "if result == 1:\n",
    "    print(\"Positive for disease\")\n",
    "elif result == -1:\n",
    "    print(\"Negative for disease\")\n",
    "else:\n",
    "    print(\"FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
